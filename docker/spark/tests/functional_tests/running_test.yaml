# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

setup:
- command:
  - docker
  - run
  - '-d'
  - '-e'
  - SPARK_LOCAL_IP=spark-master
  - '-e'
  - SPARK_WORKLOAD=master
  - -p
  - 127.0.0.1:9090:9090
  - -p
  - 127.0.0.1:7077:7077
  - --name
  - spark-master-$UNIQUE
  - "$IMAGE"
- command:
  - docker
  - run
  - -d
  - -e
  - SPARK_LOCAL_IP=spark-worker-a
  - -e
  - SPARK_LOCAL_IP=spark-worker-a
  - -e
  - SPARK_LOCAL_IP=spark-worker-a
  - -e
  - SPARK_LOCAL_IP=spark-worker-a
  - -e
  - SPARK_LOCAL_IP=spark-worker-a
  - -e
  - SPARK_LOCAL_IP=spark-worker-a
  - -e
  - SPARK_LOCAL_IP=spark-worker-a
  - --name
  - spark-worker-$UNIQUE-a
  - -p
  - 127.0.0.1:9091:8080k
  - -p
  - 127.0.0.1:7000:7000
  - "$IMAGE"
- command:
  - docker
  - run
  - -d
  - -e
  - SPARK_LOCAL_IP=spark-worker-b
  - --name
  - spark-worker-$UNIQUE-b
  - "$IMAGE"
- command: [sleep, 5s]


teardown:
- command: [docker, stop, zk-$UNIQUE-id, solr-$UNIQUE-id]
- command: [docker, rm, zk-$UNIQUE-id, solr-$UNIQUE-id]

target: solr-$UNIQUE-id
tests:
- name: Create successfully "mycollection"
  command: [solr, create_collection, -c, mycollection]
  expect:
    stdout:
      matches: "Created collection 'mycollection'"

- name: Ping successfully "mycollection"
  command: [wget, -O, -, --no-verbose, "http://localhost:8983/solr/mycollection/admin/ping?distrib=true&wt=xml"]
  expect:
    stdout:
      matches: '<str name="status">OK</str>'



# curl localhost:9090 ->
<li><strong>Alive Workers:</strong> 2</li>

# run below on master
/opt/spark/bin/spark-submit --master spark://spark-master:7077 \
	--driver-memory 1G \
	--executor-memory 1G \
	/opt/spark-apps/test_job.py | grep "app.name"

should contains -> PySpark Partition Example -> on stdout

# Overview

InfluxDB is an open source database for storing time series data, such
as data from from logging and monitoring systems, or from IoT devices.

This is a single-instance version of InfluxDB. The multi-instance version of
InfluxDB requires a commercial license.

If you are interested in the enterprise version of InfluxDB visit the
[InfluxDB website](https://www.influxdata.com/).

## About Google Click to Deploy

Popular open stacks on Kubernetes packaged by Google.

## Design

![Architecture diagram](resources/influxdb-k8s-app-arch-diagram.png)

### Solution Information

This application is single-instance InfluxDB Kubernetes solution implemented using StatefulSet object. Multi-instance implementation of InfluxDB would require commercial license from InfluxData company.

Single pod running InfluxDB container is behind Service Kubernetes object and by default it is not exposed to communication outside of Kubernetes cluster.

3 ports are configured by default:

* 8086 - for communication to InfluxDB instance and to connect Grafana monitoring application.

* 8088 - for administrative purposes (e.g. to perform InfluxDB backup)

* 2003 - for monitoring purposes (e.g. you can use this port to connect Graphite monitoring application)

### Solution Configuration

By default, this application has administrative account enabled.

You can set the admin login name using `INFLUXDB_ADMIN_USER` environment variable.

Password for the administrative account is autogenerated and stored under `INFLUXDB_ADMIN_PASSWORD` environment variable.

# Installation

## Quick install with Google Cloud Marketplace

Get up and running with a few clicks! Install this InfluxDB app to a
Google Kubernetes Engine cluster using Google Cloud Marketplace. Follow the
[on-screen instructions](https://console.cloud.google.com/marketplace/details/google/influxdb).

## Command line instructions

You can use [Google Cloud Shell](https://cloud.google.com/shell/) or a local workstation in the
further instructions.

[![Open in Cloud Shell](http://gstatic.com/cloudssh/images/open-btn.svg)](https://console.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https://github.com/GoogleCloudPlatform/click-to-deploy&cloudshell_working_dir=k8s/influxdb)

### Prerequisites

#### Set up command-line tools

You'll need the following tools in your development environment:

- [gcloud](https://cloud.google.com/sdk/gcloud/)
- [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/)
- [docker](https://docs.docker.com/install/)
- [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)

Configure `gcloud` as a Docker credential helper:

```shell
gcloud auth configure-docker
```

#### Create a Google Kubernetes Engine cluster

Create a new cluster from the command line:

```shell
export CLUSTER=influxdb-cluster
export ZONE=us-west1-a

gcloud container clusters create "$CLUSTER" --zone "$ZONE"
```

Configure `kubectl` to connect to the new cluster.

```shell
gcloud container clusters get-credentials "$CLUSTER" --zone "$ZONE"
```

#### Clone this repo

Clone this repo and the associated tools repo:

```shell
git clone --recursive https://github.com/GoogleCloudPlatform/click-to-deploy.git
```

#### Install the Application resource definition

An Application resource is a collection of individual Kubernetes components,
such as Services, Deployments, and so on, that you can manage as a group.

To set up your cluster to understand Application resources, run the following command:

```shell
kubectl apply -f "https://raw.githubusercontent.com/GoogleCloudPlatform/marketplace-k8s-app-tools/master/crd/app-crd.yaml"
```

You need to run this command once.

The Application resource is defined by the
[Kubernetes SIG-apps](https://github.com/kubernetes/community/tree/master/sig-apps) community. The source code can be found on
[github.com/kubernetes-sigs/application](https://github.com/kubernetes-sigs/application).

### Install the Application

Navigate to the `influxdb` directory:

```shell
cd click-to-deploy/k8s/influxdb
```

#### Configure the app with environment variables

Choose an instance name and
[namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/)
for the app. In most cases, you can use the `default` namespace.

```shell
export APP_INSTANCE_NAME=influxdb-1
export NAMESPACE=default
```

Configure the InfluxDB administrator account:

```shell
export INFLUXDB_ADMIN_USER=influxdb-admin
```

Configure password for InfluxDB administrator account (the value must be
encoded in base64)

```shell
export INFLUXDB_ADMIN_PASSWORD=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 12 | head -n 1 | tr -d '\n' | base64)
```

Enable Stackdriver Metrics Exporter:

> **NOTE:** Your GCP project should have Stackdriver enabled. For non-GCP clusters, export of metrics to Stackdriver is not supported yet.

```shell
export METRICS_EXPORTER_ENABLED=false
```

Configure the container image:

```shell
TAG=1.7
export IMAGE_INFLUXDB="marketplace.gcr.io/google/influxdb:${TAG}"
export IMAGE_METRICS_EXPORTER="marketplace.gcr.io/google/influxdb/prometheus-to-sd:${TAG}"
```

The images above are referenced by
[tag](https://docs.docker.com/engine/reference/commandline/tag). We recommend
that you pin each image to an immutable
[content digest](https://docs.docker.com/registry/spec/api/#content-digests).
This ensures that the installed application always uses the same images,
until you are ready to upgrade. To get the digest for the image, use the
following script:

```shell
for i in "IMAGE_INFLUXDB" "IMAGE_METRICS_EXPORTER"; do
  repo=$(echo ${!i} | cut -d: -f1);
  digest=$(docker pull ${!i} | sed -n -e 's/Digest: //p');
  export $i="$repo@$digest";
  env | grep $i;
done
```

#### Create namespace in your Kubernetes cluster

If you use a different namespace than the `default`, run the command below to create a new namespace:

```shell
kubectl create namespace "$NAMESPACE"
```

#### Expand the manifest template

Use `helm template` to expand the template. We recommend that you save the
expanded manifest file for future updates to the application.

```shell
helm template chart/influxdb \
  --name $APP_INSTANCE_NAME \
  --namespace $NAMESPACE \
  --set influxdbImage=$IMAGE_INFLUXDB \
  --set admin.user=$INFLUXDB_ADMIN_USER \
  --set admin.password=$INFLUXDB_ADMIN_PASSWORD \
  --set metrics.image=$IMAGE_METRICS_EXPORTER \
  --set metrics.enabled=$METRICS_EXPORTER_ENABLED > ${APP_INSTANCE_NAME}_manifest.yaml
```

#### Apply the manifest to your Kubernetes cluster

Use `kubectl` to apply the manifest to your Kubernetes cluster:

```shell
kubectl apply -f "${APP_INSTANCE_NAME}_manifest.yaml" --namespace "${NAMESPACE}"
```

#### View the app in the Google Cloud Console

To get the Console URL for your app, run the following command:

```shell
echo "https://console.cloud.google.com/kubernetes/application/${ZONE}/${CLUSTER}/${NAMESPACE}/${APP_INSTANCE_NAME}"
```

To view the app, open the URL in your browser.

### Access InfluxDB (internally)

You connect to InfluxDB  without exposing it to public access, using the
`influx` tool.

For information about using `influx`, and steps to upload sample data
to your instance, see the [InfluxDB Getting Started guide](https://docs.influxdata.com/influxdb/v1.7/introduction/getting-started/).

#### Connect to InfluxDB via Pod

To do this, please identify InfluxDB's Pod using the following command:

```shell
kubectl get pods -o wide -l app.kubernetes.io/name=$APP_INSTANCE_NAME --namespace "$NAMESPACE"
```

Now, you can access InfluxDB using `influx` tool

```shell
kubectl exec -it "$APP_INSTANCE_NAME-influxdb-0" --namespace "$NAMESPACE" -- influx -host localhost -port 8086 -username <admin username> -password <admin password>
```

#### Connect to InfluxDB using `kubectl port-forward` method

This method assumes that you installed `influx` tool on your local machine.
Please, refer to [InfluxDB installation instructions](https://docs.influxdata.com/influxdb/v1.7/introduction/installation/)
to learn how to do that.

You could also use a local proxy to access InfluxDB that is not exposed publicly. Run the following command in a separate background terminal:

```shell
kubectl port-forward "${APP_INSTANCE_NAME}-influxdb-0" 8086 --namespace "${NAMESPACE}"
```

Now, in your main terminal you can invoke `influx` tool as follows:

```shell
influx -host localhost -port 8086 -username <admin username> -password <admin password>
```

### Access InfluxDB (externally)

This specific InfluxDB configuration was prepared to be used as internal component of your system,
e.g. as part of your log collection system consistig of Prometheus+InfluxDB+Grafana.

It is possible to expose InfluxDB to external world - it's not recommened though to do that without securing connection to the database with SSL/TLS.

In case you would like, anyway, to expose InfluxDB solution for testing purposes (for example) you can do that in the following way:

```
kubectl patch svc "$APP_INSTANCE_NAME-influxdb-svc" \
  --namespace "$NAMESPACE" \
  --patch '{"spec": {"type": "LoadBalancer"}}'
```

> **NOTE:** It might take some time for the external IP to be provisioned.

#### Extract IP address

Get the external IP of InfluxDB instance using the following command:

```shell
INFLUXDB_IP=$(kubectl get svc $APP_INSTANCE_NAME-influxdb-svc \
  --namespace $NAMESPACE \
  --output jsonpath='{.status.loadBalancer.ingress[0].ip}')

echo $INFLUXDB_IP
```

# Application metrics

## Prometheus metrics

The application is configured to natively expose its metrics in the
[Prometheus format](https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md).
Metrics can be read on a single HTTP endpoint available at `[APP_BASE_URL]:8086/metrics`,
where `[APP_BASE_URL]` is the base URL address of the application.
For example, you can
[connect to InfluxDB using `kubectl port-forward` method](#connect-to-influxdb-using-kubectl-port-forward-method),
and then access the metrics by navigating to the [http://localhost:8086/metrics](http://localhost:8086/metrics) endpoint.

## Configuring Prometheus to collect the metrics

Prometheus can be configured to automatically collect the application's metrics.
Follow the [Configuring Prometheus documentation](https://prometheus.io/docs/introduction/first_steps/#configuring-prometheus)
to enable metrics scrapping in your Prometheus server. The detailed specification
of `<scrape_config>` used to enable the metrics collection can be found
[here](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config).

## Exporting metrics to Stackdriver

If the option to export application metrics to Stackdriver is enabled,
the deployment includes a [`prometheus-to-sd`](https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/prometheus-to-sd)
(Prometheus to Stackdriver exporter) container.
Then the metrics will be automatically exported to Stackdriver and visible in
[Stackdriver Metrics Explorer](https://cloud.google.com/monitoring/charts/metrics-explorer).

Each metric of the application will have a name starting with the application's name
(matching the variable `APP_INSTANCE_NAME` described above).

The exporting option might not be available for GKE on-prem clusters.

> Note: Please be aware that Stackdriver has [quotas](https://cloud.google.com/monitoring/quotas)
for the number of custom metrics created in a single GCP project. If the quota is met,
additional metrics will not be accepted by Stackdriver, which might cause that some metrics
from your application might not show up in the Stackdriver's Metrics Explorer.

Existing metric descriptors can be removed through
[Stackdriver's REST API](https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.metricDescriptors/delete).

# Scaling

This is a single-instance version of InfluxDB. You cannot scale it.

If you are interested in multi-instance/enterprise version of InfluxDB, please, visit [InfluxDB website](https://www.influxdata.com/).

# Backup and Restore

The following steps are based on the [InfluxDB documentation](https://docs.influxdata.com/influxdb/v1.7/administration/backup_and_restore/).

For backing up and restoring the database, use the `influxd backup` and `influxd restore` commands respectively.

To access the admin interface for InfluxDB, you need connectivity on port 8088.

Before you begin, create an `influxdb-backup` directory on your local
computer, and make sure that is empty.

## Backup InfluxDB data to your local computer

Navigate to the `influxdb/scripts` directory:

```shell
cd click-to-deploy/k8s/influxdb/scripts
```

Run the [`make_backup.sh`](scripts/make_backup.sh) script, passing the name of your InfluxDB instance as
an argument.

```shell
./make_backup.sh $APP_INSTANCE_NAME $NAMESPACE [BACKUP_FOLDER]
```

The backup is stored in the `influxdb-backup` directory on your local
computer.

## Restore InfluxDB data on running InfluxDB instance

Navigate to the `influxdb/scripts` directory:

```shell
cd click-to-deploy/k8s/influxdb/scripts
```

Run the [`make_restore.sh`](scripts/make_restore.sh) script, passing the name of your InfluxDB instance
as an argument.

```shell
./make_restore.sh $APP_INSTANCE_NAME $NAMESPACE [BACKUP_FOLDER]
```

The data is restored from the backup in the `influxdb-backup` directory on
your local computer.

# Upgrading the app

Because this is a single-instance InfluxDB solution, note that an upgrade
causes some downtime for your application. Your InfluxDB configuration and
data are retained after the upgrade.

This procudure assumes that you have a new image for InfluxDB container published and being available to your Kubernetes cluster. The new image is
used in the commands below as `[NEW_IMAGE_REFERENCE]`.

In the InfluxDB StatefulSet, modify the image used for the Pod template:

```shell
kubectl set image statefulset "$APP_INSTANCE_NAME-influxdb" \
  --namespace "$NAMESPACE" influxdb=[NEW_IMAGE_REFERENCE]
```

where `[NEW_IMAGE_REFERENCE]` is the new image.

To check the status of Pods in the StatefulSet and the progress of deploying
the new image, run the following command:

```shell
kubectl get pods -l app.kubernetes.io/name=$APP_INSTANCE_NAME --namespace "$NAMESPACE"
```

To check the current image used by Pods in the application, run the following
command:

```shell
kubectl get pods -l app.kubernetes.io/name=$APP_INSTANCE_NAME --namespace "$NAMESPACE" -o=jsonpath='{range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{", "}{end}{end}' | sort
```

# Uninstall the Application

## Using the Google Cloud Platform Console

1. In the GCP Console, open [Kubernetes Applications](https://console.cloud.google.com/kubernetes/application).

1. From the list of applications, click **InfluxDB**.

1. On the Application Details page, click **Delete**.

## Using the command line

### Prepare the environment

Set your installation name and Kubernetes namespace:

```shell
export APP_INSTANCE_NAME=influxdb-1
export NAMESPACE=default
```

### Delete the resources

> **NOTE:** We recommend to use a kubectl version that is the same as the version of your cluster. Using the same versions of kubectl and the cluster helps avoid unforeseen issues.

To delete the resources, use the expanded manifest file used for the
installation.

Run `kubectl` on the expanded manifest file:

```shell
kubectl delete -f ${APP_INSTANCE_NAME}_manifest.yaml --namespace $NAMESPACE
```

Otherwise, delete the resources using types and a label:

```shell
kubectl delete application,statefulset,service \
  --namespace $NAMESPACE \
  --selector app.kubernetes.io/name=$APP_INSTANCE_NAME
```

### Delete the persistent volumes of your installation

By design, the removal of StatefulSets in Kubernetes does not remove
PersistentVolumeClaims that were attached to their Pods. This prevents your
installations from accidentally deleting stateful data.

To remove the PersistentVolumeClaims with their attached persistent disks, run
the following `kubectl` commands:

```shell
# specify the variables values matching your installation:
export APP_INSTANCE_NAME=influxdb-1
export NAMESPACE=default

kubectl delete persistentvolumeclaims \
  --namespace $NAMESPACE \
  --selector app.kubernetes.io/name=$APP_INSTANCE_NAME
```

### Delete the GKE cluster

Optionally, if you don't need the deployed application or the GKE cluster,
delete the cluster using this command:

```
gcloud container clusters delete "$CLUSTER" --zone "$ZONE"
```

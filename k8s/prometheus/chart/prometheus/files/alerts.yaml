"\"groups\": \n- \"name\": \"kubernetes-absent\"\n  \"rules\": \n  -
    \"alert\": \"AlertmanagerDown\"\n    \"annotations\": \n      \"message\": \"Alertmanager
    has disappeared from Prometheus target discovery.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-alertmanagerdown\"\n
    \   \"expr\": |\n      absent(up{job=\"alertmanager\"} == 1)\n    \"for\": \"15m\"\n
    \   \"labels\": \n      \"severity\": \"critical\"\n  - \"alert\": \"CAdvisorDown\"\n
    \   \"annotations\": \n      \"message\": \"CAdvisor has disappeared from Prometheus
    target discovery.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-cadvisordown\"\n
    \   \"expr\": |\n      absent(up{job=\"cadvisor\"} == 1)\n    \"for\": \"15m\"\n
    \   \"labels\": \n      \"severity\": \"critical\"\n  - \"alert\": \"KubeAPIDown\"\n
    \   \"annotations\": \n      \"message\": \"KubeAPI has disappeared from Prometheus
    target discovery.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown\"\n
    \   \"expr\": |\n      absent(up{job=\"apiserver\"} == 1)\n    \"for\": \"15m\"\n
    \   \"labels\": \n      \"severity\": \"critical\"\n  - \"alert\": \"KubeDNSDown\"\n
    \   \"annotations\": \n      \"message\": \"KubeDNS has disappeared from Prometheus
    target discovery.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubednsdown\"\n
    \   \"expr\": |\n      absent(up{job=\"kube-dns\"} == 1)\n    \"for\": \"15m\"\n
    \   \"labels\": \n      \"severity\": \"critical\"\n  - \"alert\": \"KubeStateMetricsDown\"\n
    \   \"annotations\": \n      \"message\": \"KubeStateMetrics has disappeared from
    Prometheus target discovery.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatemetricsdown\"\n
    \   \"expr\": |\n      absent(up{job=\"kube-state-metrics\"} == 1)\n    \"for\":
    \"15m\"\n    \"labels\": \n      \"severity\": \"critical\"\n  - \"alert\": \"KubeletDown\"\n
    \   \"annotations\": \n      \"message\": \"Kubelet has disappeared from Prometheus
    target discovery.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown\"\n
    \   \"expr\": |\n      absent(up{job=\"kubelet\"} == 1)\n    \"for\": \"15m\"\n
    \   \"labels\": \n      \"severity\": \"critical\"\n  - \"alert\": \"NodeExporterDown\"\n
    \   \"annotations\": \n      \"message\": \"NodeExporter has disappeared from
    Prometheus target discovery.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-nodeexporterdown\"\n
    \   \"expr\": |\n      absent(up{job=\"node-exporter\"} == 1)\n    \"for\": \"15m\"\n
    \   \"labels\": \n      \"severity\": \"critical\"\n  - \"alert\": \"PrometheusDown\"\n
    \   \"annotations\": \n      \"message\": \"Prometheus has disappeared from Prometheus
    target discovery.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-prometheusdown\"\n
    \   \"expr\": |\n      absent(up{job=\"prometheus\"} == 1)\n    \"for\": \"15m\"\n
    \   \"labels\": \n      \"severity\": \"critical\"\n- \"name\": \"kubernetes-apps\"\n
    \ \"rules\": \n  - \"alert\": \"KubePodCrashLooping\"\n    \"annotations\": \n
    \     \"message\": \"{{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
    }}) is restarting {{ printf \\\"%.2f\\\" $value }} / second\"\n      \"runbook_url\":
    \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping\"\n
    \   \"expr\": |\n      rate(kube_pod_container_status_restarts_total{job=\"kube-state-metrics\"}[15m])
    > 0\n    \"for\": \"1h\"\n    \"labels\": \n      \"severity\": \"critical\"\n
    \ - \"alert\": \"KubePodNotReady\"\n    \"annotations\": \n      \"message\":
    \"{{ $labels.namespace }}/{{ $labels.pod }} is not ready.\"\n      \"runbook_url\":
    \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodnotready\"\n
    \   \"expr\": |\n      sum by (namespace, pod) (kube_pod_status_phase{job=\"kube-state-metrics\",
    phase!~\"Running|Succeeded\"}) > 0\n    \"for\": \"1h\"\n    \"labels\": \n      \"severity\":
    \"critical\"\n  - \"alert\": \"KubeDeploymentGenerationMismatch\"\n    \"annotations\":
    \n      \"message\": \"Deployment {{ $labels.namespace }}/{{ $labels.deployment
    }} generation mismatch\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentgenerationmismatch\"\n
    \   \"expr\": |\n      kube_deployment_status_observed_generation{job=\"kube-state-metrics\"}\n
    \       !=\n      kube_deployment_metadata_generation{job=\"kube-state-metrics\"}\n
    \   \"for\": \"15m\"\n    \"labels\": \n      \"severity\": \"critical\"\n  -
    \"alert\": \"KubeDeploymentReplicasMismatch\"\n    \"annotations\": \n      \"message\":
    \"Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replica mismatch\"\n
    \     \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedeploymentreplicasmismatch\"\n
    \   \"expr\": |\n      kube_deployment_spec_replicas{job=\"kube-state-metrics\"}\n
    \       !=\n      kube_deployment_status_replicas_available{job=\"kube-state-metrics\"}\n
    \   \"for\": \"15m\"\n    \"labels\": \n      \"severity\": \"critical\"\n  -
    \"alert\": \"KubeStatefulSetReplicasMismatch\"\n    \"annotations\": \n      \"message\":
    \"StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} replica mismatch\"\n
    \     \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetreplicasmismatch\"\n
    \   \"expr\": |\n      kube_statefulset_status_replicas_ready{job=\"kube-state-metrics\"}\n
    \       !=\n      kube_statefulset_status_replicas{job=\"kube-state-metrics\"}\n
    \   \"for\": \"15m\"\n    \"labels\": \n      \"severity\": \"critical\"\n  -
    \"alert\": \"KubeStatefulSetGenerationMismatch\"\n    \"annotations\": \n      \"message\":
    \"StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} generation mismatch\"\n
    \     \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubestatefulsetgenerationmismatch\"\n
    \   \"expr\": |\n      kube_statefulset_status_observed_generation{job=\"kube-state-metrics\"}\n
    \       !=\n      kube_statefulset_metadata_generation{job=\"kube-state-metrics\"}\n
    \   \"for\": \"15m\"\n    \"labels\": \n      \"severity\": \"critical\"\n  -
    \"alert\": \"KubeDaemonSetRolloutStuck\"\n    \"annotations\": \n      \"message\":
    \"Only {{$value}}% of desired pods scheduled and ready for daemon set {{$labels.namespace}}/{{$labels.daemonset}}\"\n
    \     \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetrolloutstuck\"\n
    \   \"expr\": |\n      kube_daemonset_status_number_ready{job=\"kube-state-metrics\"}\n
    \       /\n      kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"}
    * 100 < 100\n    \"for\": \"15m\"\n    \"labels\": \n      \"severity\": \"critical\"\n
    \ - \"alert\": \"KubeDaemonSetNotScheduled\"\n    \"annotations\": \n      \"message\":
    \"A number of pods of daemonset {{$labels.namespace}}/{{$labels.daemonset}} are
    not scheduled.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetnotscheduled\"\n
    \   \"expr\": |\n      kube_daemonset_status_desired_number_scheduled{job=\"kube-state-metrics\"}\n
    \       -\n      kube_daemonset_status_current_number_scheduled{job=\"kube-state-metrics\"}
    > 0\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"KubeDaemonSetMisScheduled\"\n    \"annotations\": \n      \"message\":
    \"A number of pods of daemonset {{$labels.namespace}}/{{$labels.daemonset}} are
    running where they are not supposed to run.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubedaemonsetmisscheduled\"\n
    \   \"expr\": |\n      kube_daemonset_status_number_misscheduled{job=\"kube-state-metrics\"}
    > 0\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"KubeCronJobRunning\"\n    \"annotations\": \n      \"message\":
    \"CronJob {{ $labels.namespaces }}/{{ $labels.cronjob }} is taking more than 1h
    to complete.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecronjobrunning\"\n
    \   \"expr\": |\n      time() - kube_cronjob_next_schedule_time{job=\"kube-state-metrics\"}
    > 3600\n    \"for\": \"1h\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"KubeJobCompletion\"\n    \"annotations\": \n      \"message\":
    \"Job {{ $labels.namespaces }}/{{ $labels.job }} is taking more than 1h to complete.\"\n
    \     \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobcompletion\"\n
    \   \"expr\": |\n      kube_job_spec_completions{job=\"kube-state-metrics\"} -
    kube_job_status_succeeded{job=\"kube-state-metrics\"}  > 0\n    \"for\": \"1h\"\n
    \   \"labels\": \n      \"severity\": \"warning\"\n  - \"alert\": \"KubeJobFailed\"\n
    \   \"annotations\": \n      \"message\": \"Job {{ $labels.namespaces }}/{{ $labels.job
    }} failed to complete.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubejobfailed\"\n
    \   \"expr\": |\n      kube_job_status_failed{job=\"kube-state-metrics\"}  > 0\n
    \   \"for\": \"1h\"\n    \"labels\": \n      \"severity\": \"warning\"\n- \"name\":
    \"kubernetes-resources\"\n  \"rules\": \n  - \"alert\": \"KubeCPUOvercommit\"\n
    \   \"annotations\": \n      \"message\": \"Overcommited CPU resource requests
    on Pods, cannot tolerate node failure.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit\"\n
    \   \"expr\": |\n      sum(namespace_name:kube_pod_container_resource_requests_cpu_cores:sum)\n
    \       /\n      sum(node:node_num_cpu:sum)\n        >\n      (count(node:node_num_cpu:sum)-1)
    / count(node:node_num_cpu:sum)\n    \"for\": \"5m\"\n    \"labels\": \n      \"severity\":
    \"warning\"\n  - \"alert\": \"KubeMemOvercommit\"\n    \"annotations\": \n      \"message\":
    \"Overcommited Memory resource requests on Pods, cannot tolerate node failure.\"\n
    \     \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit\"\n
    \   \"expr\": |\n      sum(namespace_name:kube_pod_container_resource_requests_memory_bytes:sum)\n
    \       /\n      sum(node_memory_MemTotal)\n        >\n      (count(node:node_num_cpu:sum)-1)\n
    \       /\n      count(node:node_num_cpu:sum)\n    \"for\": \"5m\"\n    \"labels\":
    \n      \"severity\": \"warning\"\n  - \"alert\": \"KubeCPUOvercommit\"\n    \"annotations\":
    \n      \"message\": \"Overcommited CPU resource request quota on Namespaces.\"\n
    \     \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecpuovercommit\"\n
    \   \"expr\": |\n      sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\",
    resource=\"requests.cpu\"})\n        /\n      sum(node:node_num_cpu:sum)\n        >
    1.5\n    \"for\": \"5m\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"KubeMemOvercommit\"\n    \"annotations\": \n      \"message\":
    \"Overcommited Memory resource request quota on Namespaces.\"\n      \"runbook_url\":
    \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubememovercommit\"\n
    \   \"expr\": |\n      sum(kube_resourcequota{job=\"kube-state-metrics\", type=\"hard\",
    resource=\"requests.memory\"})\n        /\n      sum(node_memory_MemTotal{job=\"node-exporter\"})\n
    \       > 1.5\n    \"for\": \"5m\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"KubeQuotaExceeded\"\n    \"annotations\": \n      \"message\":
    \"{{ printf \\\"%0.0f\\\" $value }}% usage of {{ $labels.resource }} in namespace
    {{ $labels.namespace }}.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded\"\n
    \   \"expr\": |\n      100 * kube_resourcequota{job=\"kube-state-metrics\", type=\"used\"}\n
    \       / ignoring(instance, job, type)\n      kube_resourcequota{job=\"kube-state-metrics\",
    type=\"hard\"}\n        > 90\n    \"for\": \"15m\"\n    \"labels\": \n      \"severity\":
    \"warning\"\n- \"name\": \"kubernetes-storage\"\n  \"rules\": \n  - \"alert\":
    \"KubePersistentVolumeUsageCritical\"\n    \"annotations\": \n      \"message\":
    \"The persistent volume claimed by {{ $labels.persistentvolumeclaim }} in namespace
    {{ $labels.namespace }} has {{ printf \\\"%0.0f\\\" $value }}% free.\"\n      \"runbook_url\":
    \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical\"\n
    \   \"expr\": |\n      100 * kubelet_volume_stats_available_bytes{job=\"kubelet\"}\n
    \       /\n      kubelet_volume_stats_capacity_bytes{job=\"kubelet\"}\n        <
    3\n    \"for\": \"1m\"\n    \"labels\": \n      \"severity\": \"critical\"\n  -
    \"alert\": \"KubePersistentVolumeFullInFourDays\"\n    \"annotations\": \n      \"message\":
    \"Based on recent sampling, the persistent volume claimed by {{ $labels.persistentvolumeclaim
    }} in namespace {{ $labels.namespace }} is expected to fill up within four days.\"\n
    \     \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays\"\n
    \   \"expr\": |\n      predict_linear(kubelet_volume_stats_available_bytes{job=\"kubelet\"}[1h],
    4 * 24 * 3600) < 0\n    \"for\": \"5m\"\n    \"labels\": \n      \"severity\":
    \"critical\"\n- \"name\": \"kubernetes-system\"\n  \"rules\": \n  - \"alert\":
    \"KubeNodeNotReady\"\n    \"annotations\": \n      \"message\": \"{{ $labels.node
    }} has been unready for more than an hour\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready\"\n
    \   \"expr\": |\n      kube_node_status_condition{job=\"kube-state-metrics\",condition=\"Ready\",status=\"true\"}
    == 0\n    \"for\": \"1h\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"KubeVersionMismatch\"\n    \"annotations\": \n      \"message\":
    \"There are {{ $value }} different versions of Kubernetes components running.\"\n
    \     \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeversionmismatch\"\n
    \   \"expr\": |\n      count(count(kubernetes_build_info{job!=\"kube-dns\"}) by
    (gitVersion)) > 1\n    \"for\": \"1h\"\n    \"labels\": \n      \"severity\":
    \"warning\"\n  - \"alert\": \"KubeClientErrors\"\n    \"annotations\": \n      \"message\":
    \"Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing
    {{ printf \\\"%0.0f\\\" $value }}% errors.'\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors\"\n
    \   \"expr\": |\n      sum(rate(rest_client_requests_total{code!~\"2..\"}[5m]))
    by (instance, job) * 100\n        /\n      sum(rate(rest_client_requests_total[5m]))
    by (instance, job)\n        > 1\n    \"for\": \"15m\"\n    \"labels\": \n      \"severity\":
    \"warning\"\n  - \"alert\": \"KubeClientErrors\"\n    \"annotations\": \n      \"message\":
    \"Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing
    {{ printf \\\"%0.0f\\\" $value }} errors / sec.'\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclienterrors\"\n
    \   \"expr\": |\n      sum(rate(ksm_scrape_error_total{job=\"kube-state-metrics\"}[5m]))
    by (instance, job) > 0.1\n    \"for\": \"15m\"\n    \"labels\": \n      \"severity\":
    \"warning\"\n  - \"alert\": \"KubeletTooManyPods\"\n    \"annotations\": \n      \"message\":
    \"Kubelet {{$labels.instance}} is running {{$value}} pods, close to the limit
    of 110.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods\"\n
    \   \"expr\": |\n      kubelet_running_pod_count{job=\"kubelet\"} > 100\n    \"for\":
    \"15m\"\n    \"labels\": \n      \"severity\": \"warning\"\n  - \"alert\": \"KubeAPILatencyHigh\"\n
    \   \"annotations\": \n      \"message\": \"The API server has a 99th percentile
    latency of {{ $value }} seconds for {{$labels.verb}} {{$labels.resource}}.\"\n
    \     \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh\"\n
    \   \"expr\": |\n      cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$\"}
    > 1\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"KubeAPILatencyHigh\"\n    \"annotations\": \n      \"message\":
    \"The API server has a 99th percentile latency of {{ $value }} seconds for {{$labels.verb}}
    {{$labels.resource}}.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh\"\n
    \   \"expr\": |\n      cluster_quantile:apiserver_request_latencies:histogram_quantile{job=\"apiserver\",quantile=\"0.99\",subresource!=\"log\",verb!~\"^(?:WATCH|WATCHLIST|PROXY|CONNECT)$\"}
    > 4\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\": \"critical\"\n
    \ - \"alert\": \"KubeAPIErrorsHigh\"\n    \"annotations\": \n      \"message\":
    \"API server is erroring for {{ $value }}% of requests.\"\n      \"runbook_url\":
    \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh\"\n
    \   \"expr\": |\n      sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\"}[5m]))
    without(instance, pod)\n        /\n      sum(rate(apiserver_request_count{job=\"apiserver\"}[5m]))
    without(instance, pod) * 100 > 5\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\":
    \"critical\"\n  - \"alert\": \"KubeAPIErrorsHigh\"\n    \"annotations\": \n      \"message\":
    \"API server is erroring for {{ $value }}% of requests.\"\n      \"runbook_url\":
    \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh\"\n
    \   \"expr\": |\n      sum(rate(apiserver_request_count{job=\"apiserver\",code=~\"^(?:5..)$\"}[5m]))
    without(instance, pod)\n        /\n      sum(rate(apiserver_request_count{job=\"apiserver\"}[5m]))
    without(instance, pod) * 100 > 5\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\":
    \"warning\"\n  - \"alert\": \"KubeClientCertificateExpiration\"\n    \"annotations\":
    \n      \"message\": \"Kubernetes API certificate is expiring in less than 7 days.\"\n
    \     \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration\"\n
    \   \"expr\": |\n      histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m])))
    < 604800\n    \"labels\": \n      \"severity\": \"warning\"\n  - \"alert\": \"KubeClientCertificateExpiration\"\n
    \   \"annotations\": \n      \"message\": \"Kubernetes API certificate is expiring
    in less than 1 day.\"\n      \"runbook_url\": \"https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration\"\n
    \   \"expr\": |\n      histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job=\"apiserver\"}[5m])))
    < 86400\n    \"labels\": \n      \"severity\": \"critical\"\n- \"name\": \"alertmanager.rules\"\n
    \ \"rules\": \n  - \"alert\": \"AlertmanagerFailedReload\"\n    \"annotations\":
    \n      \"description\": \"Reloading Alertmanager's configuration has failed for
    {{ $labels.namespace }}/{{ $labels.pod}}.\"\n      \"summary\": \"Alertmanager's
    configuration reload failed\"\n    \"expr\": |\n      alertmanager_config_last_reload_successful{job=\"alertmanager\"}
    == 0\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\": \"warning\"\n-
    \"name\": \"general.rules\"\n  \"rules\": \n  - \"alert\": \"TargetDown\"\n    \"annotations\":
    \n      \"description\": \"{{ $value }}% of {{ $labels.job }} targets are down.\"\n
    \     \"summary\": \"Targets are down\"\n    \"expr\": \"100 * (count(up == 0)
    BY (job) / count(up) BY (job)) > 10\"\n    \"for\": \"10m\"\n    \"labels\": \n
    \     \"severity\": \"warning\"\n  - \"alert\": \"DeadMansSwitch\"\n    \"annotations\":
    \n      \"description\": \"This is a DeadMansSwitch meant to ensure that the entire
    Alerting pipeline is functional.\"\n      \"summary\": \"Alerting DeadMansSwitch\"\n
    \   \"expr\": \"vector(1)\"\n    \"labels\": \n      \"severity\": \"none\"\n-
    \"name\": \"kube-prometheus-node-alerting.rules\"\n  \"rules\": \n  - \"alert\":
    \"NodeDiskRunningFull\"\n    \"annotations\": \n      \"description\": \"device
    {{$labels.device}} on node {{$labels.instance}} is running full within the next
    24 hours (mounted at {{$labels.mountpoint}})\"\n      \"summary\": \"Node disk
    is running full within 24 hours\"\n    \"expr\": |\n      predict_linear(node_filesystem_free{job=\"node-exporter\"}[6h],
    3600 * 24) < 0\n    \"for\": \"30m\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"NodeDiskRunningFull\"\n    \"annotations\": \n      \"description\":
    \"device {{$labels.device}} on node {{$labels.instance}} is running full within
    the next 2 hours (mounted at {{$labels.mountpoint}})\"\n      \"summary\": \"Node
    disk is running full within 2 hours\"\n    \"expr\": |\n      predict_linear(node_filesystem_free{job=\"node-exporter\"}[30m],
    3600 * 2) < 0\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\": \"critical\"\n-
    \"name\": \"prometheus.rules\"\n  \"rules\": \n  - \"alert\": \"PrometheusConfigReloadFailed\"\n
    \   \"annotations\": \n      \"description\": \"Reloading Prometheus' configuration
    has failed for {{$labels.namespace}}/{{$labels.pod}}\"\n      \"summary\": \"Reloading
    Promehteus' configuration failed\"\n    \"expr\": |\n      prometheus_config_last_reload_successful{job=\"prometheus\"}
    == 0\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"PrometheusNotificationQueueRunningFull\"\n    \"annotations\":
    \n      \"description\": \"Prometheus' alert notification queue is running full
    for {{$labels.namespace}}/{{ $labels.pod}}\"\n      \"summary\": \"Prometheus'
    alert notification queue is running full\"\n    \"expr\": |\n      predict_linear(prometheus_notifications_queue_length{job=\"prometheus\"}[5m],
    60 * 30) > prometheus_notifications_queue_capacity{job=\"prometheus\"}\n    \"for\":
    \"10m\"\n    \"labels\": \n      \"severity\": \"warning\"\n  - \"alert\": \"PrometheusErrorSendingAlerts\"\n
    \   \"annotations\": \n      \"description\": \"Errors while sending alerts from
    Prometheus {{$labels.namespace}}/{{ $labels.pod}} to Alertmanager {{$labels.Alertmanager}}\"\n
    \     \"summary\": \"Errors while sending alert from Prometheus\"\n    \"expr\":
    |\n      rate(prometheus_notifications_errors_total{job=\"prometheus\"}[5m]) /
    rate(prometheus_notifications_sent_total{job=\"prometheus\"}[5m]) > 0.01\n    \"for\":
    \"10m\"\n    \"labels\": \n      \"severity\": \"warning\"\n  - \"alert\": \"PrometheusErrorSendingAlerts\"\n
    \   \"annotations\": \n      \"description\": \"Errors while sending alerts from
    Prometheus {{$labels.namespace}}/{{ $labels.pod}} to Alertmanager {{$labels.Alertmanager}}\"\n
    \     \"summary\": \"Errors while sending alerts from Prometheus\"\n    \"expr\":
    |\n      rate(prometheus_notifications_errors_total{job=\"prometheus\"}[5m]) /
    rate(prometheus_notifications_sent_total{job=\"prometheus\"}[5m]) > 0.03\n    \"for\":
    \"10m\"\n    \"labels\": \n      \"severity\": \"critical\"\n  - \"alert\": \"PrometheusNotConnectedToAlertmanagers\"\n
    \   \"annotations\": \n      \"description\": \"Prometheus {{ $labels.namespace
    }}/{{ $labels.pod}} is not connected to any Alertmanagers\"\n      \"summary\":
    \"Prometheus is not connected to any Alertmanagers\"\n    \"expr\": |\n      prometheus_notifications_alertmanagers_discovered{job=\"prometheus\"}
    < 1\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"PrometheusTSDBReloadsFailing\"\n    \"annotations\": \n      \"description\":
    \"{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}} reload failures
    over the last four hours.\"\n      \"summary\": \"Prometheus has issues reloading
    data blocks from disk\"\n    \"expr\": |\n      increase(prometheus_tsdb_reloads_failures_total{job=\"prometheus\"}[2h])
    > 0\n    \"for\": \"12h\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"PrometheusTSDBCompactionsFailing\"\n    \"annotations\": \n      \"description\":
    \"{{$labels.job}} at {{$labels.instance}} had {{$value | humanize}} compaction
    failures over the last four hours.\"\n      \"summary\": \"Prometheus has issues
    compacting sample blocks\"\n    \"expr\": |\n      increase(prometheus_tsdb_compactions_failed_total{job=\"prometheus\"}[2h])
    > 0\n    \"for\": \"12h\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"PrometheusTSDBWALCorruptions\"\n    \"annotations\": \n      \"description\":
    \"{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead log (WAL).\"\n
    \     \"summary\": \"Prometheus write-ahead log is corrupted\"\n    \"expr\":
    |\n      tsdb_wal_corruptions_total{job=\"prometheus\"} > 0\n    \"for\": \"4h\"\n
    \   \"labels\": \n      \"severity\": \"warning\"\n  - \"alert\": \"PrometheusNotIngestingSamples\"\n
    \   \"annotations\": \n      \"description\": \"Prometheus {{ $labels.namespace
    }}/{{ $labels.pod}} isn't ingesting samples.\"\n      \"summary\": \"Prometheus
    isn't ingesting samples\"\n    \"expr\": |\n      rate(prometheus_tsdb_head_samples_appended_total{job=\"prometheus\"}[5m])
    <= 0\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\": \"warning\"\n
    \ - \"alert\": \"PrometheusTargetScapesDuplicate\"\n    \"annotations\": \n      \"description\":
    \"{{$labels.namespace}}/{{$labels.pod}} has many samples rejected due to duplicate
    timestamps but different values\"\n      \"summary\": \"Prometheus has many samples
    rejected\"\n    \"expr\": |\n      increase(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=\"prometheus\"}[5m])
    > 0\n    \"for\": \"10m\"\n    \"labels\": \n      \"severity\": \"warning\"\n"

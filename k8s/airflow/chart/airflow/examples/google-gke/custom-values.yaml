#
# NOTE:
# - This is intended to be a `custom-values.yaml` starting point for production deployment in a GKE cluster
# - We are using GKE Workload Identity rather than storing Service Account JSON tokens:
#    https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
# - Airflow requires that `explicit_defaults_for_timestamp=1` in your CloudSQL MySQL instance

# External Dependencies:
# - Git repo for DAGs:      ssh://git@repo.example.com/my-airflow-dags.git
# - CloudSQL (MySQL):       mysql.example.com:3306
# - Cloud Storage Bucket:   gs://XXXXXXXX--airflow-cluster1/
# - SMTP server:            smtpmail.example.com
# - DNS A Record:           airflow-cluster1.example.com --> XXX.XXX.XXX.XXX
# - Google Service Account: airflow-cluster1@MY_PROJECT_ID.iam.gserviceaccount.com
#
# Google IAM:
# - (Storage Bucket)
#    - gs://XXXXXXXX--airflow-cluster1
#      - roles/storage.objectAdmin        --> serviceAccount:airflow-cluster1@$MY_PROJECT_NAME.iam.gserviceaccount.com
#      - roles/storage.legacyBucketReader --> serviceAccount:airflow-cluster1@$MY_PROJECT_NAME.iam.gserviceaccount.com
# - (Service Account)
#    - airflow-cluster1@MY_PROJECT_ID.iam.gserviceaccount.com
#      - roles/iam.workloadIdentityUser   --> MY_PROJECT_NAME.svc.id.goog[airflow-cluster1/airflow]
#
# Kubernetes Resources: (see: ./examples/google-gke/k8s_resources/)
# - Namespace: airflow-cluster1
# - Secret: airflow-cluster1-fernet-key
# - Secret: airflow-cluster1-mysql-password
# - Secret: airflow-cluster1-redis-password
# - Secret: airflow-cluster1-git-secret
# - cert-manager.io/Certificate: airflow-cluster1-cert
#

###################################
# Airflow - Common Configs
###################################
airflow:
  ## the airflow executor type to use
  ##
  executor: CeleryExecutor

  ## environment variables for the web/scheduler/worker Pods (for airflow configs)
  ##
  config:
    ## security
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "False"

    ## enable SSL for webserver
    AIRFLOW__WEBSERVER__WEB_SERVER_SSL_CERT: "/opt/airflow/ssl-cert/tls.crt"
    AIRFLOW__WEBSERVER__WEB_SERVER_SSL_KEY: "/opt/airflow/ssl-cert/tls.key"

    ## dags
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "30"

    ## remote log storage
    AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
    AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "gs://XXXXXXXX--airflow-cluster1/airflow/logs"
    AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "my_gcp"

    ## email
    AIRFLOW__EMAIL__EMAIL_BACKEND: "airflow.utils.email.send_email_smtp"
    AIRFLOW__SMTP__SMTP_HOST: "smtpmail.example.com"
    AIRFLOW__SMTP__SMTP_MAIL_FROM: "admin@airflow-cluster1.example.com"
    AIRFLOW__SMTP__SMTP_PORT: "25"
    AIRFLOW__SMTP__SMTP_SSL: "False"
    AIRFLOW__SMTP__SMTP_STARTTLS: "False"

    ## domain used in airflow emails
    AIRFLOW__WEBSERVER__BASE_URL: "https://airflow-cluster1.example.com/"

  ## a list of initial users to create
  ##
  users:
    - username: admin
      password: admin
      role: Admin
      email: admin@example.com
      firstName: admin
      lastName: admin

  ## if we update users or just create them the first time (lookup by `username`)
  ##
  usersUpdate: false

  ## a list of initial connections to create
  ##
  connections:
    ## see docs: https://airflow.apache.org/docs/apache-airflow-providers-google/stable/connections/gcp.html
    - id: my_gcp
      type: google_cloud_platform
      description: my GCP connection
      extra: |-
        { "extra__google_cloud_platform__num_retries": "5" }

  ## a list of initial variables to create
  ##
  variables:
    - key: "environment"
      value: "prod"

  ## a list of initial pools to create
  ##
  pools:
    - name: "pool_1"
      slots: 5
      description: "example pool with 5 slots"
    - name: "pool_2"
      slots: 10
      description: "example pool with 10 slots"

  ## extra environment variables for the web/scheduler/worker (AND flower) Pods
  ##
  extraEnv:
    - name: AIRFLOW__CORE__FERNET_KEY
      valueFrom:
        secretKeyRef:
          name: airflow-cluster1-fernet-key
          key: value

###################################
# Airflow - Scheduler Configs
###################################
scheduler:
  ## resource requests/limits for the scheduler Pod
  ##
  resources:
    requests:
      cpu: "1000m"
      memory: "512Mi"

  ## configs for the log-cleanup sidecar of the scheduler
  ##
  ## NOTE:
  ## - helps prevent excessive log buildup by regularly deleting old files
  ##
  logCleanup:
    ## if the log-cleanup sidecar is enabled
    ##
    enabled: true

    ## resource requests/limits for the log-cleanup container
    ##
    resources:
      requests:
        cpu: "10m"
        memory: "32Mi"

    ## the number of minutes to retain log files (by last-modified time)
    ##
    retentionMinutes: 21600

###################################
# Airflow - WebUI Configs
###################################
web:
  ## configs to generate webserver_config.py
  ##
  webserverConfig:
    ## the full text value to mount as the webserver_config.py file
    ##
    stringOverride: |-
      from flask_appbuilder.security.manager import AUTH_DB

      # use embedded DB for auth
      AUTH_TYPE = AUTH_DB

  ## the number of web Pods to run
  ##
  replicas: 1

  ## resource requests/limits for the airflow web Pods
  ##
  resources:
    requests:
      cpu: "200m"
      memory: "900Mi"

  ## configs for the Service of the web Pods
  ##
  service:
    annotations:
      cloud.google.com/load-balancer-type: "Internal"
    type: LoadBalancer
    externalPort: 443
    loadBalancerIP: XXX.XXX.XXX.XXX
    loadBalancerSourceRanges: []

  ## extra VolumeMounts for the web Pods
  ##
  extraVolumeMounts:
    - name: ssl-cert
      mountPath: /opt/airflow/ssl-cert
      readOnly: true

  ## extra Volumes for the web Pods
  ##
  extraVolumes:
    - name: ssl-cert
      secret:
        secretName: airflow-cluster1-cert

###################################
# Airflow - Worker Configs
###################################
workers:
  ## if the airflow workers StatefulSet should be deployed
  ##
  enabled: true

  ## the number of workers Pods to run
  ##
  replicas: 2

  ## resource requests/limits for the airflow worker Pods
  ##
  resources:
    requests:
      cpu: "256m"
      memory: "2Gi"

  ## configs for the PodDisruptionBudget of the worker StatefulSet
  ##
  podDisruptionBudget:
    ## if a PodDisruptionBudget resource is created for the worker StatefulSet
    ##
    enabled: true

    ## the maximum unavailable pods/percentage for the worker StatefulSet
    ##
    ## NOTE:
    ## - prevents loosing more than 20% of current worker task slots in a voluntary
    ##   disruption
    ##
    maxUnavailable: "20%"

  ## configs for the HorizontalPodAutoscaler of the worker Pods
  ##
  autoscaling:
    enabled: true
    maxReplicas: 8
    metrics:
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

  ## configs for the celery worker Pods
  ##
  celery:
    ## if we should wait for tasks to finish before SIGTERM of the celery worker
    ##
    gracefullTermination: true

    ## how many seconds to wait for tasks to finish before SIGTERM of the celery worker
    ##
    ## WARNING:
    ## - GKE cluster-autoscaler will not respect graceful termination period over 10min
    ## NOTE:
    ## - this gives any running tasks AT MOST 9min to complete
    ##
    gracefullTerminationPeriod: 540

  ## how many seconds to wait after SIGTERM before SIGKILL of the celery worker
  ##
  terminationPeriod: 60

  ## configs for the log-cleanup sidecar of the worker Pods
  ##
  ## NOTE:
  ## - helps prevent excessive log buildup by regularly deleting old files
  ##
  logCleanup:
    ## if the log-cleanup sidecar is enabled
    ##
    enabled: true

    ## resource requests/limits for the log-cleanup container
    ##
    ## WARNING:
    ## - you MUST SPECIFY a resource request for logCleanup if using `workers.autoscaling`
    ##
    resources:
      requests:
        cpu: "10m"
        memory: "32Mi"

    ## the number of minutes to retain log files (by last-modified time)
    ##
    retentionMinutes: 21600

###################################
# Airflow - Triggerer Configs
###################################
triggerer:
  ## if the airflow triggerer should be deployed
  ##
  ## WARNING:
  ## - the triggerer component was added in airflow 2.2.0
  ## - if `airflow.legacyCommands` is `true` the triggerer will NOT be deployed
  ##
  enabled: true

  ## the number of triggerer Pods to run
  ##
  replicas: 1

  ## resource requests/limits for the triggerer Pods
  ##
  resources:
    requests:
      cpu: "256m"
      memory: "2Gi"

  ## maximum number of triggers each triggerer will run at once (sets `AIRFLOW__TRIGGERER__DEFAULT_CAPACITY`)
  ##
  capacity: 1000

###################################
# Airflow - Flower Configs
###################################
flower:
  ## if the Flower UI should be deployed
  ##
  enabled: true

  ## resource requests/limits for the flower Pods
  ##
  resources:
    requests:
      cpu: "10m"
      memory: "64Mi"

  ## configs for the Service of the flower Pods
  ##
  service:
    annotations: {}
    type: ClusterIP
    externalPort: 5555
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

###################################
# Airflow - Logs Configs
###################################
logs:
  ## configs for the logs PVC
  ##
  persistence:
    ## if a persistent volume is mounted at `logs.path`
    ##
    enabled: false

###################################
# Airflow - DAGs Configs
###################################
dags:
  ## configs for the git-sync sidecar (https://github.com/kubernetes/git-sync)
  ##
  gitSync:
    ## if the git-sync sidecar container is enabled
    ##
    enabled: true

    ## resource requests/limits for the git-sync container
    ##
    ## WARNING:
    ## - you MUST SPECIFY a resource request for gitSync if using `workers.autoscaling`
    ##
    resources:
      requests:
        cpu: "50m"
        memory: "64Mi"

    ## the url of the git repo
    ##
    repo: "git@repo.example.com/my-airflow-dags.git"

    ## the git branch to check out
    ##
    branch: master

    ## the git revision (tag or hash) to check out
    ##
    revision: HEAD

    ## the number of seconds between syncs
    ##
    syncWait: 60

    ## the name of a pre-created Secret with git ssh credentials
    ##
    sshSecret: "airflow-cluster1-git-secret"

    ## the key in `dags.gitSync.sshSecret` with your ssh-key file
    ##
    sshSecretKey: id_rsa

###################################
# Kubernetes - RBAC
###################################
rbac:
  ## if Kubernetes RBAC resources are created
  ##
  create: true

###################################
# Kubernetes - Service Account
###################################
serviceAccount:
  ## if a Kubernetes ServiceAccount is created
  ##
  create: true

  ## the name of the ServiceAccount
  ##
  name: "airflow"

  ## annotations for the ServiceAccount
  ##
  annotations:
    iam.gke.io/gcp-service-account: airflow-cluster1@MY_PROJECT_ID.iam.gserviceaccount.com

###################################
# Database - PostgreSQL Chart
###################################
postgresql:
  ## if the `stable/postgresql` chart is used
  ##
  enabled: false

###################################
# Database - External Database
###################################
externalDatabase:
  ## the type of external database: {mysql,postgres}
  ##
  type: mysql

  ## the host of the external database
  ##
  host: mysql.airflow-cluster1.example.com

  ## the port of the external database
  ##
  port: 3306

  ## the database/scheme to use within the the external database
  ##
  database: airflow_cluster1

  ## the user of the external database
  ##
  user: airflow_cluster1

  ## the name of a pre-created secret containing the external database password
  ##
  passwordSecret: airflow-cluster1-mysql-password

  ## the key within `externalDatabase.passwordSecret` containing the password string
  ##
  passwordSecretKey: mysql-password

###################################
# Database - Redis Chart
###################################
redis:
  ## if the `stable/redis` chart is used
  ##
  enabled: true

  ## the name of a pre-created secret containing the redis password
  ##
  existingSecret: "airflow-cluster1-redis-password"

  ## the key in `redis.existingSecret` containing the password string
  ##
  existingSecretPasswordKey: "redis-password"

  ## configs for redis cluster mode
  ##
  cluster:
    ## if redis runs in cluster mode
    ##
    enabled: false

    ## the number of redis slaves
    ##
    slaveCount: 1

  ## configs for the redis master
  ##
  master:
    ## resource requests/limits for the master Pod
    ##
    resources:
      requests:
        cpu: "10m"
        memory: "32Mi"

    ## configs for the PVC of the redis master
    ##
    persistence:
      ## use a PVC to persist data
      ##
      enabled: false

  ## configs for the redis slaves
  ##
  slave:
    ## resource requests/limits for the slave Pods
    ##
    resources:
      requests:
        cpu: "10m"
        memory: "32Mi"

    ## configs for the PVC of the redis slaves
    ##
    persistence:
      ## use a PVC to persist data
      ##
      enabled: false
